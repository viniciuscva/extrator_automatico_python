{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1626667853567,
     "user": {
      "displayName": "CARLOS VINICIUS ALVES MINERVINO",
      "photoUrl": "",
      "userId": "13290675844861944858"
     },
     "user_tz": 180
    },
    "id": "HSHihLsk9NJT"
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aNq3_tRF9NJY"
   },
   "outputs": [],
   "source": [
    "#ETAPA DE PADRONIZAÇÃO DAS IMAGENS\n",
    "directory = '/home/vinicius/Documents/tcc/teste1'\n",
    "os.system(f'if [ ! -d \"{directory}/JPG\" ]; then mkdir {directory}/JPG; fi;')\n",
    "for filename in os.listdir(f'{directory}/PDF'):\n",
    "    os.system(f'convert -density 300 {directory}/PDF/{filename} -background white -alpha remove -flatten -alpha off -resize 992x1403 {directory}/JPG/{filename.split(\".\")[0]}.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WZh0mrn29NJa"
   },
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "import cv2\n",
    "import imutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i3EDVozL9NJc"
   },
   "outputs": [],
   "source": [
    "#ETAPA DE ALINHAMENTO DAS IMAGENS COM O TEMPLATE \n",
    "# código-fonte escrito por Adrian Rosebrock, disponível em https://www.pyimagesearch.com/2020/08/31/image-alignment-and-registration-with-opencv/\n",
    "import numpy as np\n",
    "import imutils\n",
    "import cv2\n",
    "def align_images(image, template, maxFeatures=500, keepPercent=0.2,\n",
    "    debug=False):\n",
    "    # convert both the input image and template to grayscale\n",
    "    imageGray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    templateGray = cv2.cvtColor(template, cv2.COLOR_BGR2GRAY)\n",
    "        # use ORB to detect keypoints and extract (binary) local\n",
    "    # invariant features\n",
    "    orb = cv2.ORB_create(maxFeatures)\n",
    "    (kpsA, descsA) = orb.detectAndCompute(imageGray, None)\n",
    "    (kpsB, descsB) = orb.detectAndCompute(templateGray, None)\n",
    "    # match the features\n",
    "    method = cv2.DESCRIPTOR_MATCHER_BRUTEFORCE_HAMMING\n",
    "    matcher = cv2.DescriptorMatcher_create(method)\n",
    "    matches = matcher.match(descsA, descsB, None)\n",
    "    # sort the matches by their distance (the smaller the distance,\n",
    "    # the \"more similar\" the features are)\n",
    "    matches = sorted(matches, key=lambda x:x.distance)\n",
    "    # keep only the top matches\n",
    "    keep = int(len(matches) * keepPercent)\n",
    "    matches = matches[:keep]\n",
    "    # check to see if we should visualize the matched keypoints\n",
    "    if debug:\n",
    "        matchedVis = cv2.drawMatches(image, kpsA, template, kpsB,\n",
    "            matches, None)\n",
    "        matchedVis = imutils.resize(matchedVis, width=1000)\n",
    "        cv2.imshow(\"Matched Keypoints\", matchedVis)\n",
    "        cv2.waitKey(0)\n",
    "        # allocate memory for the keypoints (x, y)-coordinates from the\n",
    "    # top matches -- we'll use these coordinates to compute our\n",
    "    # homography matrix\n",
    "    ptsA = np.zeros((len(matches), 2), dtype=\"float\")\n",
    "    ptsB = np.zeros((len(matches), 2), dtype=\"float\")\n",
    "    # loop over the top matches\n",
    "    for (i, m) in enumerate(matches):\n",
    "        # indicate that the two keypoints in the respective images\n",
    "        # map to each other\n",
    "        ptsA[i] = kpsA[m.queryIdx].pt\n",
    "        ptsB[i] = kpsB[m.trainIdx].pt\n",
    "        # compute the homography matrix between the two sets of matched\n",
    "    # points\n",
    "    (H, mask) = cv2.findHomography(ptsA, ptsB, method=cv2.RANSAC)\n",
    "    # use the homography matrix to align the images\n",
    "    (h, w) = template.shape[:2]\n",
    "    aligned = cv2.warpPerspective(image, H, (w, h))\n",
    "    # return the aligned image\n",
    "    return aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-MSHZsmg9NJe",
    "outputId": "ee94fffd-d9d5-44e1-f252-8a06f7359f8e"
   },
   "outputs": [],
   "source": [
    "# adaptação de código escrito por Adrian Rosebrock, disponível em https://www.pyimagesearch.com/2020/08/31/image-alignment-and-registration-with-opencv/\n",
    "directory = '/home/vinicius/Documents/tcc/teste1/JPG'\n",
    "os.system(f'if [ ! -d \"{directory}/Alinhadas\" ]; then mkdir {directory}/Alinhadas; fi;')\n",
    "for filename in os.listdir(directory):\n",
    "    if not filename.endswith('.jpg'):\n",
    "        continue\n",
    "    # load the input image and template from disk\n",
    "    print(\"[INFO] loading images...\")\n",
    "    image = cv2.imread(f'{directory}/{filename}')\n",
    "    template = cv2.imread('/home/vinicius/Documents/tcc/teste1/template.png')\n",
    "    # align the images\n",
    "    print(\"[INFO] aligning images...\")\n",
    "    aligned = align_images(image, template, debug=False)\n",
    "    # resize both the aligned and template images so we can easily\n",
    "    # visualize them on our screen\n",
    "    #aligned = imutils.resize(aligned, width=700)\n",
    "    #template = imutils.resize(template, width=700)\n",
    "    # our first output visualization of the image alignment will be a\n",
    "    # side-by-side comparison of the output aligned image and the\n",
    "    # template\n",
    "    #stacked = np.hstack([aligned, template])\n",
    "    # our second image alignment visualization will be *overlaying* the\n",
    "    # aligned image on the template, that way we can obtain an idea of\n",
    "    # how good our image alignment is\n",
    "    #overlay = template.copy()\n",
    "    #output = aligned.copy()\n",
    "    cv2.imwrite(f'{directory}/Alinhadas/{filename.split(\".\")[0]}_alinhada.jpg', aligned)\n",
    "    #cv2.addWeighted(overlay, 0.5, output, 0.5, 0, output)\n",
    "    # show the two output image alignment visualizations\n",
    "    #cv2.imshow(\"Image Alignment Stacked\", stacked)\n",
    "    #cv2.imwrite('.stacked.png', stacked)\n",
    "    #cv2.imshow(\"Image Alignment Overlay\", output)\n",
    "    #cv2.imwrite('output.png', output)\n",
    "    #cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g2v7NLW69bOo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "testes1_tcc2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
